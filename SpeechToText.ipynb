{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time speech to text python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project would not make any API calls and essentially we should be able to get real time speech to text conversion by just using our computer. Three parts to the project :\n",
    "1. Create Widgets that start and stop recording\n",
    "2. Use pyaudio to record microphone in background\n",
    "3. vosk lib for speech recognition --> then add the output to the jupyter widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from /Users/akashnikam/.cache/vosk/vosk-model-en-us-0.22/rnnlm/final.raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93559513de64485db352d9c3eebe38e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', icon='microphone', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4ad9a425814c3fbb72e54e4d1c66d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', icon='stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a5bb0a2d6a49618824e39619edc753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Part 1 Import Modules and Create Widgets\n",
    "\n",
    "import pyaudio\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "from queue import Queue\n",
    "from ipywidgets import Button, Output\n",
    "from IPython.display import display\n",
    "from threading import Thread\n",
    "import json, time\n",
    "\n",
    "# Create the output widget\n",
    "output = Output()\n",
    "# Create the start and stop buttons\n",
    "start_button = Button(description='Start Recording', icon='microphone')\n",
    "stop_button = Button(description='Stop Recording', icon='stop')\n",
    "\n",
    "# Add the click event handlers to the buttons\n",
    "start_button.on_click(lambda _: start_recording())\n",
    "stop_button.on_click(lambda _: stop_recording())\n",
    "\n",
    "# Create queues for communication between threads\n",
    "'''\n",
    "These two threads will run in the background simultaneously. messages will send out signal to the microphone to stop recording if messages are empty.\n",
    "recordings is the collection of the audio from record_microphone that gets pulled by speech recognition function. Threads are used for making this simultaneous.\n",
    "'''\n",
    "messages = Queue()\n",
    "recordings = Queue()\n",
    "\n",
    "# Function to start recording and transcribing\n",
    "def start_recording():\n",
    "    messages.put(True)\n",
    "    \n",
    "    record = Thread(target=record_microphone)\n",
    "    record.start()\n",
    "    \n",
    "    transcribe = Thread(target=speech_recognition, args=(output,))\n",
    "    transcribe.start()\n",
    "    with output:\n",
    "        print(\"Starting...\")\n",
    "# Function to stop recording\n",
    "def stop_recording():\n",
    "    with output:\n",
    "        messages.queue.clear()  # Clear the messages queue\n",
    "        print(\"Stopped.\")\n",
    "\n",
    "\n",
    "## Part 2 Recording from Microphone using Pyaudio\n",
    "\n",
    "# Define the constants for speech recognition\n",
    "CHANNELS = 1\n",
    "FRAME_RATE = 16000\n",
    "RECORD_SECONDS = 8 #records for 8 seconds and then sends it for transcription. you can change this and play around but CPU usage will also change.\n",
    "AUDIO_FORMAT = pyaudio.paInt16\n",
    "\n",
    "'''\n",
    "This commented piece of code prints out all devices connected to your computer. You can check the index of the microphone and note it. \n",
    "We use it furhter down in record_microphone.\n",
    "'''\n",
    "# p = pyaudio.PyAudio()\n",
    "# #we need to check how many audio devices are connected to our system\n",
    "\n",
    "# for i in range(p.get_device_count()):\n",
    "#     print(p.get_device_info_by_index(i))\n",
    "\n",
    "# p.terminate()\n",
    "\n",
    "\n",
    "# Function for recording the microphone\n",
    "def record_microphone(chunk=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(\n",
    "        format=AUDIO_FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=FRAME_RATE,\n",
    "        input=True,\n",
    "        input_device_index=1,\n",
    "        frames_per_buffer=chunk\n",
    "    )\n",
    "    frames = []\n",
    "\n",
    "    while not messages.empty():\n",
    "        \n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "        #seems like max frames size is 32\n",
    "        if len(frames) >= (FRAME_RATE * RECORD_SECONDS) / chunk:\n",
    "            recordings.put(frames.copy())\n",
    "            frames = []\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "## Part 3 Recognizing live speech with vosk\n",
    "          \n",
    "# Create the Vosk model and recognizer\n",
    "model = Model(model_name='vosk-model-en-us-0.22') #this model is standard, you can change it acc. to the language.\n",
    "rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "rec.SetWords(True)\n",
    "\n",
    "\n",
    "# Function for speech recognition\n",
    "def speech_recognition(output):\n",
    "    \n",
    "    while not messages.empty():\n",
    "        frames = recordings.get()\n",
    "        rec.AcceptWaveform(b''.join(frames))\n",
    "        result = rec.Result()\n",
    "        text = json.loads(result)['text']\n",
    "        output.append_stdout(f'{text} \\n')\n",
    "            \n",
    "\n",
    "\n",
    "# Display the buttons and the output widget\n",
    "display(start_button, stop_button, output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It's important to note that Python has a Global Interpreter Lock (GIL), which means that only one thread can execute Python bytecode at a time. As a result, threads in Python are not suitable for CPU-bound tasks that require heavy computational work. For CPU-bound tasks, you may want to consider using multiprocessing or other concurrent programming approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
